# =============================================================
# app.py â€” Streamlit UI for LangGraph RAG
# PDF yÃ¼kle â†’ Graphâ€‘1 (ingest) â†’ RAG toggle â†’ Graphâ€‘2 (cevap + metrik + kaynak)
# Bu dosya sadece UI akÄ±ÅŸÄ±nÄ± yÃ¶netir; iÅŸ mantÄ±ÄŸÄ± graph1/graph2â€™dedir.
# =============================================================
import os
# Streamlit: web arayÃ¼z bileÅŸenleri
import streamlit as st

# Backend fonksiyonlarÄ±: Graphâ€‘1 (ingest), Graphâ€‘2 (RAG) ve dÃ¼z LLM Ã§aÄŸrÄ±sÄ±
from graph1_pdf_upload import run_graph1
from graph2 import run_graph2
from services import llm_generate

# --- Helper: her tÃ¼rlÃ¼ LLM Ã§Ä±ktÄ±sÄ±nÄ± dÃ¼z metne Ã§evir ---
def _as_text(x) -> str:
    """AIMessage / dict / repr('content="..." additional_kwargs=...') â†’ plain text"""
    # 1) AIMessage benzeri obje
    if hasattr(x, "content"):
        try:
            return x.content
        except Exception:
            pass
    # 2) dict formatÄ±
    if isinstance(x, dict) and "content" in x:
        try:
            return str(x.get("content", ""))
        except Exception:
            pass
    # 3) str'e Ã§evir ve repr kalÄ±bÄ±ndan iÃ§eriÄŸi ayÄ±kla
    s = str(x)
    try:
        import re
        m = re.search(r"content=([\\\'\"])(.*?)\1\s+additional_kwargs=", s, re.DOTALL)
        if m:
            return m.group(2)
    except Exception:
        pass
    return s

# Sayfa baÅŸlÄ±ÄŸÄ±/ikon ve geniÅŸ layout ayarÄ±
st.set_page_config(page_title="LangGraph RAG Evaluator", page_icon="ğŸ§ ", layout="wide")
# Uygulama baÅŸlÄ±ÄŸÄ±
st.title("ğŸ§  LangGraph RAG Evaluator")

# ---- Session State ----
# Chat ve Ã§alÄ±ÅŸma durumu burada tutulur; tarayÄ±cÄ± yenilense bile korunur
# VarsayÄ±lan durum anahtarÄ± oluÅŸtur
if "doc_id" not in st.session_state:
    st.session_state.doc_id = None
# VarsayÄ±lan durum anahtarÄ± oluÅŸtur
if "vector_ready" not in st.session_state:
    st.session_state.vector_ready = False
# VarsayÄ±lan durum anahtarÄ± oluÅŸtur
if "rag_mode" not in st.session_state:
    st.session_state.rag_mode = False
# VarsayÄ±lan durum anahtarÄ± oluÅŸtur
if "chat" not in st.session_state:
    st.session_state.chat = []  # list of dicts: {role: "user"|"assistant", content: str, meta: Optional}

# --- Sidebar: PDF yÃ¼kleme ve RAG anahtarÄ± ---
with st.sidebar:
    # Sidebar baÅŸlÄ±ÄŸÄ±
    st.header("ğŸ“„ PDF & RAG")
    # Tek PDF dosyasÄ± yÃ¼kleme alanÄ±
    uploaded_pdf = st.file_uploader("PDF yÃ¼kle", type=["pdf"], accept_multiple_files=False)

    # Dosya geldiÄŸinde: diske kaydet + Graphâ€‘1â€™i Ã§alÄ±ÅŸtÄ±r
    if uploaded_pdf is not None:
        # GeÃ§ici isimle kaydet (tek dosya iÅŸ akÄ±ÅŸÄ± iÃ§in yeterli)
        temp_path = "uploaded.pdf"
        # PDF iÃ§eriÄŸini yerel dosyaya yaz
        with open(temp_path, "wb") as f:
            f.write(uploaded_pdf.read())
        # Ingest akÄ±ÅŸÄ±nÄ± gÃ¶sterge ile Ã§alÄ±ÅŸtÄ±r
        with st.spinner("PDF iÅŸleniyor (Graphâ€‘1)â€¦"):
            # Graphâ€‘1: PDF â†’ chunk â†’ vektÃ¶r DB; geri doc_id dÃ¶ner
            doc_id = run_graph1(temp_path)
        # UI durumunu gÃ¼ncelle (RAG iÃ§in gerekli)
        st.session_state.doc_id = doc_id
        st.session_state.vector_ready = True
        # KullanÄ±cÄ±ya baÅŸarÄ± mesajÄ±
        st.success(f"PDF yÃ¼klendi ve vektÃ¶rlendi âœ… doc_id={doc_id}")

    # GÃ¶rsel ayraÃ§
    st.divider()
    # RAG modunu aÃ§/kapat (cevaplar PDF baÄŸlamÄ±na gÃ¶re olsun mu?)
    st.session_state.rag_mode = st.toggle("RAG modu (PDF'e dayalÄ± cevap)", value=st.session_state.rag_mode)
    # VektÃ¶r DB hazÄ±r deÄŸilse uyar (PDF yÃ¼klenmedi)
    if st.session_state.rag_mode and not st.session_state.vector_ready:
        st.warning("RAG iÃ§in Ã¶nce PDF yÃ¼klemelisin.")

# Ana sohbet alanÄ±
chat_container = st.container()
# Sohbet geÃ§miÅŸini ve yeni mesajlarÄ± bu container iÃ§inde gÃ¶ster
with chat_container:
    # Ã–nceki konuÅŸmalarÄ± sÄ±rayla Ã§iz
    for msg in st.session_state.chat:
        # Mesaj balonu (user/assistant)
        with st.chat_message(msg["role"]):
            # Mesaj metnini yazdÄ±r (sadece dÃ¼z metin; ham obje ise ayÄ±kla)
            _content = _as_text(msg.get("content"))
            st.markdown(_content)
            # (Varsa) bu mesaja ait kaynaklar ve metrikler
            meta = msg.get("meta")
            if meta and msg["role"] == "assistant":
                # Kaynaklar: sayfa/baÅŸlÄ±k/snippet ve reâ€‘rank skoru
                sources = meta.get("sources") or []
                metrics = meta.get("metrics") or {}
                if sources:
                    with st.expander("ğŸ”— Sources / Metadata"):
                        # Her kaynak iÃ§in ayrÄ±ntÄ±
                        for s in sources:
                            st.markdown(
                                f"- **page**: {s.get('page')} | **chunk**: `{s.get('chunk_id')}` | **score**: {s.get('rerank_score'):.3f}\n\n"
                                f"  **title/section**: {s.get('title') or s.get('section') or '-'}\n\n"
                                f"  _{s.get('snippet', '')}_\n"
                            )
                # Metrikler: context/answer relevance ve groundedness
                if metrics:
                    # 3 sÃ¼tunda metrik gÃ¶sterimi
                    cols = st.columns(3)
                    keys = ["context_relevance", "answer_relevance", "groundedness"]
                    labels = ["Context Relevance", "Answer Relevance", "Groundedness"]
                    for i, k in enumerate(keys):
                        with cols[i]:
                            st.metric(labels[i], f"{metrics.get(k, 0.0):.3f}")

    # KullanÄ±cÄ±dan yeni mesaj al
    user_input = st.chat_input("MesajÄ±nÄ± yazâ€¦")

    # Mesaj gelirse: Ã¶nce geÃ§miÅŸe ekle, sonra RAG mi dÃ¼z LLM mi karar ver
    if user_input:
        # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe yaz
        st.session_state.chat.append({"role": "user", "content": user_input})

        # RAG modu aÃ§Ä±kken PDF tabanlÄ± cevap Ã¼ret
        if st.session_state.rag_mode:
            # PDF/vektÃ¶r hazÄ±r deÄŸilse bilgi mesajÄ± gÃ¶nder
            if not st.session_state.vector_ready:
                assistant_text = "RAG modu aÃ§Ä±k ama PDF yok. LÃ¼tfen Ã¶nce PDF yÃ¼kle."
                st.session_state.chat.append({"role": "assistant", "content": assistant_text})
            else:
                # Asistan balonu: RAG cevabÄ± ve meta gÃ¶sterimi
                with st.chat_message("assistant"):
                    # Graphâ€‘2: retrieve â†’ generate â†’ evaluate (+ retry)
                    with st.spinner("RAG (Graphâ€‘2) Ã§alÄ±ÅŸÄ±yorâ€¦"):
                        result = run_graph2(user_input, doc_id=st.session_state.doc_id, max_retries=2)
                    # Modele ait yanÄ±t metni
                    assistant_text = _as_text(result.get("answer", "(boÅŸ)"))
                    st.markdown(assistant_text)
                    # Bu tura ait kaynak ve metrikleri anÄ±nda gÃ¶ster
                    sources = result.get("sources", [])
                    metrics = result.get("metrics", {})
                    if sources:
                        with st.expander("ğŸ”— Sources / Metadata"):
                            for s in sources:
                                st.markdown(
                                    f"- **page**: {s.get('page')} | **chunk**: `{s.get('chunk_id')}` | **score**: {s.get('rerank_score'):.3f}\n\n"
                                    f"  **title/section**: {s.get('title') or s.get('section') or '-'}\n\n"
                                    f"  _{s.get('snippet', '')}_\n"
                                )
                    if metrics:
                        cols = st.columns(3)
                        keys = ["context_relevance", "answer_relevance", "groundedness"]
                        labels = ["Context Relevance", "Answer Relevance", "Groundedness"]
                        for i, k in enumerate(keys):
                            with cols[i]:
                                st.metric(labels[i], f"{metrics.get(k, 0.0):.3f}")
                    # GeÃ§miÅŸte de aynÄ± tur iÃ§in kaynak/metrik sakla
                    meta = {"sources": result.get("sources", []), "metrics": result.get("metrics", {})}
                    st.session_state.chat.append({"role": "assistant", "content": assistant_text, "meta": meta})
        # RAG kapalÄ±yken: dÃ¼z LLM sohbeti (services.llm_generate)
        else:
            # Basit sistem mesajÄ± (role)
            system = "You are a helpful assistant."
            # LLMâ€™den yanÄ±t al (Together/Ollama via services)
            with st.chat_message("assistant"):
                with st.spinner("LLM yanÄ±t Ã¼retiyorâ€¦"):
                    assistant_text = _as_text(llm_generate(system, user_input, max_tokens=512))
                st.markdown(assistant_text)
                # Asistan yanÄ±tÄ±nÄ± geÃ§miÅŸe ekle
                st.session_state.chat.append({"role": "assistant", "content": assistant_text})

# Bilgilendirme notu: RAG aÃ§Ä±k/kapalÄ± davranÄ±ÅŸÄ±
st.caption("RAG aÃ§Ä±kken cevaplar PDF baÄŸlamÄ±yla Ã¼retilir ve kaynak/metric gÃ¶rÃ¼ntÃ¼lenir. RAG kapalÄ±yken dÃ¼z LLM sohbet edilir.")